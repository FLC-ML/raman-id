{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ramen ID Kaitlyn",
      "provenance": [],
      "authorship_tag": "ABX9TyPlRUy3cescs18qGcT0DWXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FLC-ML/raman-id/blob/master/Ramen_ID_Kaitlyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgT7Lr9tLZs9",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSRFo5OLXgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezh_mXFfKvEd",
        "colab_type": "text"
      },
      "source": [
        "ResNet Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFfKnvZeKTbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.layers import BatchNormalization, ZeroPadding1D, Conv1D, AveragePooling1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import Activation, Dense,  Flatten, Input, add, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# define resnet class\n",
        "class ResNet:\n",
        "  @staticmethod\n",
        "  # create one residual block\n",
        "  def residual_module(data, K, convLayers, chanDim=-1, reduce=False, reg=0.0001, bnEps=2e-5, bnMom=0.9, dropout=0.8, kernel_size=1):\n",
        "    K = K*0.25\n",
        "    shortcut = data\n",
        "    x = data\n",
        "\n",
        "    print(\"Resnet Block\")\n",
        "    for i in range(convLayers):\n",
        "      x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "      x = Activation(\"relu\")(x)\n",
        "      \n",
        "      if (i == convLayers - 1):\n",
        "        K = K*4\n",
        "        x = Conv1D(int(K), kernel_size, use_bias=False, kernel_regularizer=l2(reg))(x)\n",
        "      elif (i == int(convLayers/2)):\n",
        "        x = Conv1D(int(K), kernel_size, use_bias=False, kernel_regularizer=l2(reg))(x)\n",
        "      else:\n",
        "        x = Conv1D(int(K), kernel_size, use_bias=False, kernel_regularizer=l2(reg))(x)\n",
        "      if (dropout is not None):\n",
        "        x = Dropout(dropout)(x)\n",
        "     \n",
        "      # print(\"**PostConvolution\", x.shape)\n",
        "      print(\"*ConvGroup\", x.shape, K)\n",
        "\n",
        "    if reduce:\n",
        "      # shortcut = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(shortcut)\n",
        "      shortcut = Conv1D(K, kernel_size, use_bias=False, kernel_regularizer=l2(reg))(shortcut)\n",
        "    print(\"*Shortcut shape\", shortcut.shape)\n",
        "\n",
        "    x = add([x, shortcut])\n",
        "\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  # build a model out of resblocks\n",
        "  def build(inputShape, resLayers, classes, convLayers, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dropout=None, kernel_size=1):\n",
        "    chanDim = -1\n",
        "    inputs = Input(shape=inputShape)\n",
        "\n",
        "    # initial batchnorm and convolutional layer\n",
        "    x = BatchNormalization(axis=chanDim, epsilon = bnEps, momentum=bnMom)(inputs)\n",
        "    x = Conv1D(filters[0], 5, strides=2, use_bias=False, padding=\"same\", kernel_regularizer=l2(reg))(inputs)\n",
        "    \n",
        "    x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "       \n",
        "    x = ResNet.residual_module(data=x, K=filters[1], convLayers=convLayers, reduce=True, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "    for i in range(0, resLayers - 1):\n",
        "      x = ResNet.residual_module(data=x, K=filters[1], convLayers=convLayers, bnEps=bnEps, bnMom=bnMom)\n",
        "    \n",
        "    x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = AveragePooling1D(8)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
        "    x = Activation(\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "    return model\n",
        "\n",
        "  @staticmethod\n",
        "  def pretrain(save_directory, features, labels, verbose=1,\n",
        "                                   batch_size=32, \n",
        "                                   epochs=10, \n",
        "                                   filters=(64, 100), \n",
        "                                   resLayers=1, \n",
        "                                   convLayers=1, \n",
        "                                   learningRate=0.01,   #decreased to 0.01 from 0.1\n",
        "                                   dropout=0.8,\n",
        "                                   kernel_size=1):\n",
        "    X = features\n",
        "    y = labels\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01)\n",
        "\n",
        "\n",
        "    # add depth channel\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # input shape is everything except for the number of samples \n",
        "    # number of classes is the number of unique items in y\n",
        "    in_shape = X_train.shape[1:]\n",
        "    n_classes = len(np.unique(y_train))\n",
        "\n",
        "    # build model\n",
        "    model = ResNet.build(inputShape=in_shape, classes=n_classes, convLayers=convLayers, resLayers=resLayers, filters=filters, dropout=dropout)\n",
        "\n",
        "    # compile, train model\n",
        "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer=Adam(learningRate), metrics = ['accuracy'])\n",
        "    \n",
        "    ####### ADDED THIS #######\n",
        "    tf.keras.utils.plot_model(model, to_file = 'Pretrained_Model_Architecture.png', show_shapes=True, show_layer_names=False)\n",
        "    ##########################\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs, verbose=verbose)\n",
        "\n",
        "    # test accuracy\n",
        "    accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n",
        "    print(\"(pretraining) Testing accuracy from within X_reference.npy and y_reference.npy:\", accuracy[1])\n",
        "\n",
        "    # Save the weights\n",
        "    model.save(save_directory)\n",
        "\n",
        "    # return model\n",
        "    return model\n",
        "\n",
        "  @staticmethod\n",
        "  def finetune(save_directory, features, labels, verbose=1,\n",
        "               learningRate=1e-4,   #increased to 1e-4 from 1e-5\n",
        "               batch_size=10, \n",
        "               epochs=10):\n",
        "    X = features\n",
        "    y = labels\n",
        "\n",
        "    # create \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "    \n",
        "    model = load_model(save_directory)\n",
        "  \n",
        "    model.compile(optimizer=Adam(learningRate),  # Very low learning rate\n",
        "    loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
        "    loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n",
        "    print(\"(finetuning) Testing accuracy from within X_fine and y_fine: \", acc)\n",
        "  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-5yo5jzK_fx",
        "colab_type": "text"
      },
      "source": [
        "Pretraining Hyperparameters and Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Ndt6ON1Q6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrain hyperparameters\n",
        "batch_size = 100\n",
        "epochs = 30\n",
        "verbose = 1\n",
        "filters = (64, 100)\n",
        "resLayers = 1\n",
        "convLayers = 1\n",
        "learningRate = 0.001\n",
        "dropout = 0.8\n",
        "kernel_size = 1\n",
        "\n",
        "\n",
        "# save_directory is the path where the model will be saved after pretraining \n",
        "# and the path where the finetuning model will read from\n",
        "save_directory = '/content/drive/My Drive/ML Group/Individual Work/Kaitlyn/Saved Models/base.h5' \n",
        "\n",
        "\n",
        "# load data (specific to google drive)\n",
        "X_test = np.load('/content/drive/My Drive/ML Group/Datasets/data/X_test.npy')\n",
        "y_test = np.load('/content/drive/My Drive/ML Group/Datasets/data/y_test.npy')\n",
        "\n",
        "X = np.load('/content/drive/My Drive/ML Group/Datasets/data/X_reference.npy')\n",
        "y = np.load('/content/drive/My Drive/ML Group/Datasets/data/y_reference.npy')\n",
        "\n",
        "X_fine = np.load('/content/drive/My Drive/ML Group/Datasets/data/X_finetune.npy')\n",
        "y_fine = np.load('/content/drive/My Drive/ML Group/Datasets/data/y_finetune.npy')\n",
        "\n",
        "print(\"X Reference Dataset Shape: \", X.shape)\n",
        "print(\"y Reference Dataset Shape: \", y.shape)\n",
        "print(\"X Test Dataset Shape: \", X_test.shape)\n",
        "print(\"y Test Dataset Shape: \", y_test.shape)\n",
        "print(\"X Finetuning Dataset Shape: \", X_fine.shape)\n",
        "print(\"y Finetuning Dataset Shape: \", y_fine.shape)\n",
        "\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "pretrained_model = ResNet.pretrain(save_directory=save_directory, features=X, labels=y, verbose=verbose,\n",
        "                                   batch_size=batch_size, \n",
        "                                   epochs=epochs, \n",
        "                                   filters=filters, \n",
        "                                   resLayers=resLayers, \n",
        "                                   convLayers=convLayers, \n",
        "                                   learningRate=learningRate,\n",
        "                                   dropout=dropout,\n",
        "                                   kernel_size=kernel_size)\n",
        "\n",
        "pre_loss, pre_acc = pretrained_model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n",
        "print(\"Testing loss, accuracy after pretraining from X_test and y_test:\", pre_loss, pre_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfWgWY1wK13i",
        "colab_type": "text"
      },
      "source": [
        "Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CruboOEw1SKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# finetune hyperparameters\n",
        "ft_batch_size = 10\n",
        "ft_epochs = 65\n",
        "ft_learningRate = 1e-4    #increased to 1e-4 from 1e-5\n",
        "\n",
        "finetuned_model = ResNet.finetune(save_directory=save_directory, features=X_fine, labels=y_fine, epochs=ft_epochs, learningRate=ft_learningRate, batch_size=ft_batch_size)\n",
        "\n",
        "loss, acc = finetuned_model.evaluate(X_test, y_test, batch_size=ft_batch_size, verbose=verbose)\n",
        "print(\"Pretrained accuracy: \", pre_acc)\n",
        "print(\"Fintuned accuracy: \", acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8rJSaSv--Tb",
        "colab_type": "text"
      },
      "source": [
        "Pretrained Accuracy: 54.69%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qulidb87_F7-",
        "colab_type": "text"
      },
      "source": [
        "Finetuned Accuracy: 83.23%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQq7iZr0eQnr",
        "colab_type": "text"
      },
      "source": [
        "Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp5ltErx6Nd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = finetuned_model.predict(x = X_test)\n",
        "predIndx = np.argmax(y_pred, axis = 1)\n",
        "print(\"Predictions Shape:\", y_pred.shape)\n",
        "print(\"Classes:\", y_pred[0].shape)\n",
        "print(len(y_pred), \"Predictions Made\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2zsHKE4h9vM",
        "colab_type": "text"
      },
      "source": [
        "Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI4VXlBKiCiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get labels and order\n",
        "\n",
        "ORDER = [16, 17, 14, 18, 15, 20, 21, 24, 23, 26, 27, 28, 29, 25, 6, 7, 5, 3, 4,\n",
        "         9, 10, 2, 8, 11, 22, 19, 12, 13, 0, 1]\n",
        "\n",
        "STRAINS = {}\n",
        "STRAINS[0] = \"C. albicans\"\n",
        "STRAINS[1] = \"C. glabrata\"\n",
        "STRAINS[2] = \"K. aerogenes\"\n",
        "STRAINS[3] = \"E. coli 1\"\n",
        "STRAINS[4] = \"E. coli 2\"\n",
        "STRAINS[5] = \"E. faecium\"\n",
        "STRAINS[6] = \"E. faecalis 1\"\n",
        "STRAINS[7] = \"E. faecalis 2\"\n",
        "STRAINS[8] = \"E. cloacae\"\n",
        "STRAINS[9] = \"K. pneumoniae 1\"\n",
        "STRAINS[10] = \"K. pneumoniae 2\"\n",
        "STRAINS[11] = \"P. mirabilis\"\n",
        "STRAINS[12] = \"P. aeruginosa 1\"\n",
        "STRAINS[13] = \"P. aeruginosa 2\"\n",
        "STRAINS[14] = \"MSSA 1\"\n",
        "STRAINS[15] = \"MSSA 3\"\n",
        "STRAINS[16] = \"MRSA 1 (isogenic)\"\n",
        "STRAINS[17] = \"MRSA 2\"\n",
        "STRAINS[18] = \"MSSA 2\"\n",
        "STRAINS[19] = \"S. enterica\"\n",
        "STRAINS[20] = \"S. epidermidis\"\n",
        "STRAINS[21] = \"S. lugdunensis\"\n",
        "STRAINS[22] = \"S. marcescens\"\n",
        "STRAINS[23] = \"S. pneumoniae 2\"\n",
        "STRAINS[24] = \"S. pneumoniae 1\"\n",
        "STRAINS[25] = \"S. sanguinis\"\n",
        "STRAINS[26] = \"Group A Strep.\"\n",
        "STRAINS[27] = \"Group B Strep.\"\n",
        "STRAINS[28] = \"Group C Strep.\"\n",
        "STRAINS[29] = \"Group G Strep.\"\n",
        "\n",
        "C_albicans = np.where(y==0)\n",
        "C_glabrata = np.where(y==1)\n",
        "K_aerogenes = np.where(y==2)\n",
        "E_coli_1 = np.where(y==3)\n",
        "E_coli_2 = np.where(y==4)\n",
        "E_faecium = np.where(y==5)\n",
        "E_faecalis_1 = np.where(y==6)\n",
        "E_faecalis_2 = np.where(y==7)\n",
        "E_cloacae = np.where(y==8)\n",
        "K_pneumoniae_1 = np.where(y==9)\n",
        "K_pneumoniae_2 = np.where(y==10)\n",
        "P_mirabilis = np.where(y==11)\n",
        "P_aeruginosa_1 = np.where(y==12)\n",
        "P_aeruginosa_2 = np.where(y==13)\n",
        "MSSA_1 = np.where(y==14)\n",
        "MSSA_3 = np.where(y==15)\n",
        "MRSA_1_iso = np.where(y==16)\n",
        "MRSA_2 = np.where(y==17)\n",
        "MSSA_2 = np.where(y==18)\n",
        "S_enterica = np.where(y==19)\n",
        "S_epidermidis = np.where(y==20)\n",
        "S_lugdunensis = np.where(y==21)\n",
        "S_marcescens = np.where(y==22)\n",
        "S_pneumoniae_2 = np.where(y==23)\n",
        "S_pneumoniae_1 = np.where(y==24)\n",
        "S_sanguinis = np.where(y==25)\n",
        "Group_A_Strep = np.where(y==26)\n",
        "Group_B_Strep = np.where(y==27)\n",
        "Group_C_Strep = np.where(y==28)\n",
        "Group_G_Strep = np.where(y==29)\n",
        "\n",
        "label = [STRAINS[i] for i in ORDER]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am1eBH7Rh17i",
        "colab_type": "text"
      },
      "source": [
        "Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSviB8Oih5Fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "c6bb439d-2838-44b6-8705-287c92a91a77"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "import pandas\n",
        "\n",
        "cr = classification_report(y_test, predIndx, target_names = label)\n",
        "print(cr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "MRSA 1 (isogenic)       0.99      0.99      0.99       100\n",
            "           MRSA 2       0.99      1.00      1.00       100\n",
            "           MSSA 1       0.70      0.80      0.74       100\n",
            "           MSSA 2       0.89      1.00      0.94       100\n",
            "           MSSA 3       0.82      0.55      0.66       100\n",
            "   S. epidermidis       0.99      1.00      1.00       100\n",
            "   S. lugdunensis       1.00      0.21      0.35       100\n",
            "  S. pneumoniae 1       0.55      0.87      0.68       100\n",
            "  S. pneumoniae 2       0.74      0.71      0.72       100\n",
            "   Group A Strep.       0.73      0.73      0.73       100\n",
            "   Group B Strep.       0.91      0.92      0.92       100\n",
            "   Group C Strep.       0.58      0.40      0.47       100\n",
            "   Group G Strep.       0.72      0.87      0.79       100\n",
            "     S. sanguinis       1.00      0.71      0.83       100\n",
            "    E. faecalis 1       0.90      1.00      0.95       100\n",
            "    E. faecalis 2       0.77      0.70      0.73       100\n",
            "       E. faecium       0.70      0.85      0.77       100\n",
            "        E. coli 1       0.97      0.61      0.75       100\n",
            "        E. coli 2       0.98      0.93      0.95       100\n",
            "  K. pneumoniae 1       0.97      0.95      0.96       100\n",
            "  K. pneumoniae 2       1.00      1.00      1.00       100\n",
            "     K. aerogenes       0.73      0.95      0.83       100\n",
            "       E. cloacae       0.63      0.82      0.71       100\n",
            "     P. mirabilis       0.87      0.75      0.81       100\n",
            "    S. marcescens       0.70      0.88      0.78       100\n",
            "      S. enterica       0.86      0.89      0.87       100\n",
            "  P. aeruginosa 1       0.94      0.96      0.95       100\n",
            "  P. aeruginosa 2       0.99      1.00      1.00       100\n",
            "      C. albicans       0.93      0.95      0.94       100\n",
            "      C. glabrata       0.89      0.97      0.93       100\n",
            "\n",
            "         accuracy                           0.83      3000\n",
            "        macro avg       0.85      0.83      0.82      3000\n",
            "     weighted avg       0.85      0.83      0.82      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoXumxcfMAA8",
        "colab_type": "text"
      },
      "source": [
        "Create Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoKJaIYguCsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot confusion matrix\n",
        "\n",
        "p_test = pretrained_model.predict(X_test).argmax(axis=1)\n",
        "cm = confusion_matrix(y_test, p_test, labels = ORDER)\n",
        "\n",
        "plt.figure(figsize = (15,15))\n",
        "cm = 100*cm / cm.sum(axis = 1)[:,np.newaxis]\n",
        "\n",
        "ax = sns.heatmap(cm, annot = True, cmap = \"YlGnBu\", fmt = '0.0f', \n",
        "                 xticklabels = label, yticklabels = label)\n",
        "#ax.xaxis.tick_top()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.title('Normalized Confusion Matrix', fontsize = 16)\n",
        "plt.xlabel('Predicted', fontsize = 14)\n",
        "plt.ylabel('True', fontsize = 14)\n",
        "plt.savefig('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}